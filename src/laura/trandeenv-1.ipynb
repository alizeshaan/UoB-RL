{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"implements a dqn agent to maximise the reward using the gym_tranding environment\"\"\"\n",
    "\n",
    "\"\"\" definitions:\n",
    "Environment : Asset Market in this case the BCT to USD market (using Binance data)\n",
    "Agent : Trading Algorithm that uses the environment to perform trading actions and maximise reward\n",
    "State : Historical Price, Volume & Volatility Data of the asset\n",
    "Agent Actions : Buy, Sell, Do Nothing (Hold)\n",
    "Reward : Next period returns based on the action chosen and final return of the portfolio. \n",
    "Profit : The final return of the portfolio at the end of the trading period\"\"\"\n",
    "\n",
    "import gym_trading_env\n",
    "import gym_trading_env.utils.history as history\n",
    "from gym_trading_env.utils.history import History\n",
    "from gym_trading_env.utils import history\n",
    "from gym_trading_env.environments import TradingEnv\n",
    "from gym_trading_env.utils.download_data import download_data\n",
    "import gymnasium as gym\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download BTC/USDT historical data from Binance and stores it to directory ./data/binance-BTCUSDT-1h.pkl\n",
    "download_data(exchange_names = [\"binance\"],\n",
    "    symbols= [\"BTC/USDT\"],\n",
    "    timeframe= \"1h\",\n",
    "    dir = \"/Users/laura.barbosa/Downloads/\",\n",
    "    since= datetime.datetime(year= 2020, month= 1, day=1),\n",
    ")\n",
    "# Import your fresh data\n",
    "df = pd.read_pickle(\"/Users/laura.barbosa/Downloads/binance-BTCUSDT-1h.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>date_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>7195.24</td>\n",
       "      <td>7196.25</td>\n",
       "      <td>7175.46</td>\n",
       "      <td>7177.02</td>\n",
       "      <td>511.814901</td>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:00:00</th>\n",
       "      <td>7176.47</td>\n",
       "      <td>7230.00</td>\n",
       "      <td>7175.71</td>\n",
       "      <td>7216.27</td>\n",
       "      <td>883.052603</td>\n",
       "      <td>2020-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 02:00:00</th>\n",
       "      <td>7215.52</td>\n",
       "      <td>7244.87</td>\n",
       "      <td>7211.41</td>\n",
       "      <td>7242.85</td>\n",
       "      <td>655.156809</td>\n",
       "      <td>2020-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 03:00:00</th>\n",
       "      <td>7242.66</td>\n",
       "      <td>7245.00</td>\n",
       "      <td>7220.00</td>\n",
       "      <td>7225.01</td>\n",
       "      <td>783.724867</td>\n",
       "      <td>2020-01-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 04:00:00</th>\n",
       "      <td>7225.00</td>\n",
       "      <td>7230.00</td>\n",
       "      <td>7215.03</td>\n",
       "      <td>7217.27</td>\n",
       "      <td>467.812578</td>\n",
       "      <td>2020-01-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-09 09:00:00</th>\n",
       "      <td>29836.39</td>\n",
       "      <td>29836.40</td>\n",
       "      <td>29779.13</td>\n",
       "      <td>29781.78</td>\n",
       "      <td>1065.099370</td>\n",
       "      <td>2023-08-09 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-09 10:00:00</th>\n",
       "      <td>29781.77</td>\n",
       "      <td>29854.71</td>\n",
       "      <td>29740.01</td>\n",
       "      <td>29842.45</td>\n",
       "      <td>1033.090010</td>\n",
       "      <td>2023-08-09 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-09 11:00:00</th>\n",
       "      <td>29842.45</td>\n",
       "      <td>29914.19</td>\n",
       "      <td>29795.20</td>\n",
       "      <td>29888.32</td>\n",
       "      <td>1416.399900</td>\n",
       "      <td>2023-08-09 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-09 12:00:00</th>\n",
       "      <td>29888.32</td>\n",
       "      <td>30094.40</td>\n",
       "      <td>29866.01</td>\n",
       "      <td>30076.32</td>\n",
       "      <td>3003.085990</td>\n",
       "      <td>2023-08-09 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-09 13:00:00</th>\n",
       "      <td>30076.33</td>\n",
       "      <td>30160.00</td>\n",
       "      <td>29946.06</td>\n",
       "      <td>29946.94</td>\n",
       "      <td>2488.291250</td>\n",
       "      <td>2023-08-09 14:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31566 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close       volume  \\\n",
       "date_open                                                                  \n",
       "2020-01-01 00:00:00   7195.24   7196.25   7175.46   7177.02   511.814901   \n",
       "2020-01-01 01:00:00   7176.47   7230.00   7175.71   7216.27   883.052603   \n",
       "2020-01-01 02:00:00   7215.52   7244.87   7211.41   7242.85   655.156809   \n",
       "2020-01-01 03:00:00   7242.66   7245.00   7220.00   7225.01   783.724867   \n",
       "2020-01-01 04:00:00   7225.00   7230.00   7215.03   7217.27   467.812578   \n",
       "...                       ...       ...       ...       ...          ...   \n",
       "2023-08-09 09:00:00  29836.39  29836.40  29779.13  29781.78  1065.099370   \n",
       "2023-08-09 10:00:00  29781.77  29854.71  29740.01  29842.45  1033.090010   \n",
       "2023-08-09 11:00:00  29842.45  29914.19  29795.20  29888.32  1416.399900   \n",
       "2023-08-09 12:00:00  29888.32  30094.40  29866.01  30076.32  3003.085990   \n",
       "2023-08-09 13:00:00  30076.33  30160.00  29946.06  29946.94  2488.291250   \n",
       "\n",
       "                             date_close  \n",
       "date_open                                \n",
       "2020-01-01 00:00:00 2020-01-01 01:00:00  \n",
       "2020-01-01 01:00:00 2020-01-01 02:00:00  \n",
       "2020-01-01 02:00:00 2020-01-01 03:00:00  \n",
       "2020-01-01 03:00:00 2020-01-01 04:00:00  \n",
       "2020-01-01 04:00:00 2020-01-01 05:00:00  \n",
       "...                                 ...  \n",
       "2023-08-09 09:00:00 2023-08-09 10:00:00  \n",
       "2023-08-09 10:00:00 2023-08-09 11:00:00  \n",
       "2023-08-09 11:00:00 2023-08-09 12:00:00  \n",
       "2023-08-09 12:00:00 2023-08-09 13:00:00  \n",
       "2023-08-09 13:00:00 2023-08-09 14:00:00  \n",
       "\n",
       "[31566 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#investigate the data to check for any preprocessing needed\n",
    "\n",
    "#make the df to have a different amount of rows\n",
    "#df = df[:3000]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating features based on suggestion from library\n",
    "def preprocess(df : pd.DataFrame):\n",
    "    df[\"feature_close\"] = df[\"close\"].pct_change()\n",
    "    df[\"feature_open\"] = df[\"open\"]/df[\"close\"]\n",
    "    df[\"feature_high\"] = df[\"high\"]/df[\"close\"]\n",
    "    df[\"feature_low\"] = df[\"low\"]/df[\"close\"]\n",
    "    df[\"feature_volume\"] = df[\"volume\"] / df[\"volume\"].rolling(7*24).max()\n",
    "    df.dropna(inplace= True)\n",
    "    return df\n",
    "\n",
    "#call the function to generate the features\n",
    "df = preprocess(df)\n",
    "\n",
    "def reward_function(history):\n",
    "    \"\"\"Create reward function with the history object, it compares the two more recent portfolio valuations \n",
    "    and take the log of the ratio this is to incentivize the agent to trade in the future \"\"\"\n",
    "\n",
    "    portfolio_valuations = history[\"portfolio_valuation\"]\n",
    "    current_valuation = portfolio_valuations[-1]\n",
    "    previous_valuation = portfolio_valuations[-2]\n",
    "\n",
    "    if current_valuation > 0 and previous_valuation > 0:\n",
    "        reward = np.log(current_valuation / previous_valuation)  # Set a positive reward in case of positive valuations, use the log to scale the reward\n",
    "    else:\n",
    "        reward = 0  # Set a default reward in case of non-positive valuations\n",
    "    \n",
    "    return reward\n",
    "\n",
    "#set the additional required parameters for the reward function\n",
    "portfolio = gym_trading_env.utils.portfolio.Portfolio(1000, 0.01/100, 0.0003/100)   \n",
    "history = gym_trading_env.utils.history.History(reward_function) #reward_function = reward_function \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.03517865,  0.9660927 ,  1.0027491 , ...,  0.98936653,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.00722264,  1.0071211 ,  1.0092248 , ...,  0.65946186,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.01102985,  1.011154  ,  1.017323  , ...,  0.3584984 ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.00225875,  0.99774635,  1.0008335 , ...,  0.20948946,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.00293313,  1.0029417 ,  1.0039066 , ...,  0.19243005,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.00208737,  0.997916  ,  1.0016868 , ...,  0.19647782,\n",
       "         -1.        , -1.0041895 ]], dtype=float32),\n",
       " -0.0021925556622356323,\n",
       " False,\n",
       " False,\n",
       " {'idx': 730,\n",
       "  'step': 1,\n",
       "  'date': numpy.datetime64('2020-02-07T09:00:00.000000000'),\n",
       "  'position_index': 0,\n",
       "  'position': -1,\n",
       "  'real_position': -1.0041895031898356,\n",
       "  'data_volume': 1713.890217,\n",
       "  'data_open': 9773.07,\n",
       "  'data_date_close': Timestamp('2020-02-07 10:00:00'),\n",
       "  'data_close': 9793.48,\n",
       "  'data_low': 9773.05,\n",
       "  'data_high': 9810.0,\n",
       "  'portfolio_valuation': 997.8098462321806,\n",
       "  'portfolio_distribution_asset': 0,\n",
       "  'portfolio_distribution_fiat': 1999.8000199980002,\n",
       "  'portfolio_distribution_borrowed_asset': 0.1023116571233429,\n",
       "  'portfolio_distribution_borrowed_fiat': 0,\n",
       "  'portfolio_distribution_interest_asset': 3.0693497137002866e-07,\n",
       "  'portfolio_distribution_interest_fiat': 0.0,\n",
       "  'reward': -0.0021925556622356323})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#define the environmnet\n",
    "env = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = df, # defining the state space which includes: open, high, low, close, volume, feature_close, feature_open, feature_high, feature_low\n",
    "        positions = [ -1, 0, 1], #definiing the action space -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate = 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "        windows = 730, # Number of previous timesteps to consider, setting it to 1 month\n",
    "        initial_position = 0, # Initial position\n",
    "        verbose = 1,\n",
    "        reward_function = reward_function\n",
    "    )\n",
    "\n",
    "#add History to the environment\n",
    "history = History(max_size=10000)\n",
    "\n",
    "#add metrics to the environment\n",
    "env.unwrapped.add_metric('Position Changes', lambda history : np.sum(np.diff(history['position']) != 0) )\n",
    "env.unwrapped.add_metric('Episode Length', lambda history : len(history['position']) )\n",
    "env.unwrapped.add_metric('Profit', lambda history: history['portfolio_valuation'][-1] - history['portfolio_valuation'][0])\n",
    "env.unwrapped.add_metric('Profit (%)', lambda history: (history['portfolio_valuation'][-1] - history['portfolio_valuation'][0])/history['portfolio_valuation'][0] )\n",
    "env.unwrapped.add_metric('Portfolio Value', lambda history : history['portfolio_valuation'][-1] )\n",
    "env.unwrapped.add_metric('Portfolio Value Change', lambda history : history['portfolio_valuation'][-1] - history['portfolio_valuation'][0] )\n",
    "env.unwrapped.add_metric('Portfolio Value Change (%)', lambda history : (history['portfolio_valuation'][-1] - history['portfolio_valuation'][0])/history['portfolio_valuation'][0] )\n",
    "\n",
    "\n",
    "#check the inititial state of the environment\n",
    "env.reset()\n",
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an episode until it ends :\n",
    "#done, truncated = False, False\n",
    "#observation, info = env.reset()\n",
    "#while not done and not truncated:\n",
    "    # Pick a position by its index in your position list (=[-1, 0, 1])....usually something like : position_index = your_policy(observation)\n",
    "#    position_index = env.action_space.sample() # At every timestep, pick a random position index from your position list (=[-1, 0, 1])\n",
    "#    observation, reward, done, truncated, info = env.step(position_index)\n",
    "\n",
    "#print(len(env.env._price_array))\n",
    "#print(env.env._nb_features)\n",
    "#print(env.env._price_array[0])\n",
    "#print(env.env._info_array[0])\n",
    "\n",
    "\n",
    "#env.reset()\n",
    "#print(env.step(0))\n",
    "#env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 107.91%   |   Portfolio Return : -100.49%   |   Position Changes : 41   |   Episode Length : 7505   |   Profit : -1004.9019276286164   |   Profit (%) : -1.0049019276286164   |   Portfolio Value : -4.901927628616379   |   Portfolio Value Change : -1004.9019276286164   |   Portfolio Value Change (%) : -1.0049019276286164   |   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create a memory replay buffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size # maximum size of memory\n",
    "        self.mem_cntr = 0 # memory counter\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape)) # state memory\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape)) # new state memory\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32) # action memory (now an array of integers)\n",
    "        self.reward_memory = np.zeros(self.mem_size) # reward memory\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool_) # terminal state memory      \n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        # store transition in memory\n",
    "        index = self.mem_cntr % self.mem_size # index of memory\n",
    "        self.state_memory[index] = state # store state\n",
    "        self.new_state_memory[index] = state_ # store new state\n",
    "        self.action_memory[index] = action # store action as an integer (no need for one-hot encoding)\n",
    "        self.reward_memory[index] = reward # store reward\n",
    "        self.terminal_memory[index] = done # store terminal state\n",
    "        self.mem_cntr += 1 # increment memory counter\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        # sample a batch of data from memory\n",
    "        max_mem = min(self.mem_cntr, self.mem_size) # get the maximum memory\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False) # get a random batch of memory\n",
    "        states = np.array([self.state_memory[i] for i in batch])  # Convert to a NumPy array\n",
    "        states_ = np.array([self.new_state_memory[i] for i in batch])  # Convert to a NumPy array\n",
    "        actions = self.action_memory[batch] # get the actions (now an array of integers)\n",
    "        rewards = self.reward_memory[batch] # get the rewards\n",
    "        terminal = self.terminal_memory[batch] # get the terminal states\n",
    "\n",
    "        return states, actions, rewards, states_, terminal\n",
    "\n",
    "        \n",
    "#build a deep q network agent class\n",
    "class Agent:\n",
    "    def __init__(self, lr, gamma, n_actions, epsilon, batch_size, input_dims, mem_size=100000, eps_min=0.01, eps_dec=5e-7, replace=1000, chkpt_dir='tmp/dqn'):\n",
    "        self.action_space = list(range(n_actions)) #action space\n",
    "        self.gamma = gamma #discount factor\n",
    "        self.epsilon = epsilon #epsilon\n",
    "        self.eps_min = eps_min #minimum epsilon\n",
    "        self.eps_dec = eps_dec #epsilon decrement\n",
    "        self.replace_target_cnt = replace #replace target network counter (how often we replace the target network)\n",
    "        self.batch_size = batch_size #batch size for training\n",
    "        self.learn_step_counter = 0 #learn step counter\n",
    "        self.memory = ReplayBuffer(mem_size, [730, 7], n_actions) #create a memory replay buffer using the size, the input shape, and the number of actions\n",
    "        self.q_eval = DeepQNetwork(n_actions=n_actions, input_dims=input_dims, #create a deep q network for evaluation\n",
    "                                   lr=lr, name='q_eval', chkpt_dir=chkpt_dir)\n",
    "        self.q_next = DeepQNetwork(n_actions=n_actions, input_dims=input_dims, #create a deep q network for next states\n",
    "                                   lr=lr, name='q_next', chkpt_dir=chkpt_dir)   \n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        \"\"\"We need to store the trasiitons of the agent in memory, so states and actions are preserved\"\"\"\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        \"\"\"We need to choose an action. We need to convert the observation\n",
    "        into a tensor and send it to the GPU. We then need to generate a random\n",
    "        number between 0 and 1. If the random number is less than epsilon, then\n",
    "        we need to return a random action. Otherwise, we need to return the\n",
    "        action with the highest Q-value.\"\"\"\n",
    "\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            #if random number is less than epsilon, return a random action\n",
    "            return np.random.choice(self.action_space)\n",
    "        \n",
    "        #otherwise, return the action with the highest Q-value\n",
    "        state = T.tensor(observation, dtype=T.float32).to(self.q_eval.device) #convert observation to a tensor of type float32\n",
    "        actions = self.q_eval.forward(state) #get the actions from the q_eval network\n",
    "        action = T.argmax(actions).item() #get the action with the highest Q-value\n",
    "\n",
    "        # Ensure action is within the valid range of actions\n",
    "        action = max(-1, min(2, action)) \n",
    "        return action\n",
    "\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        \"\"\"As per the DQN paper, we need to replace the target network, \n",
    "        which  is the q_next network, with the q_eval network\"\"\"\n",
    "        if self.learn_step_counter % self.replace_target_cnt == 0: #if the learn step counter is divisible by the replace target counter\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict()) #replace the q_next network with the q_eval network   \n",
    "\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        \"\"\"We need to decrement epsilon so that we don't get stuck in a local\n",
    "        minimum. We also need to make sure that the minimum epsilon is\n",
    "        not below zero.\n",
    "        \"\"\"\n",
    "        #decrement epsilon\n",
    "        self.epsilon = self.epsilon - self.eps_dec  \n",
    "        if self.epsilon > self.eps_min:\n",
    "            self.epsilon = self.epsilon - self.eps_dec\n",
    "        else:\n",
    "            self.eps_min\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"In order to learn, we need to sample a batch of data from memory.\n",
    "        We then need to convert the data into tensors and send them to the GPU, \n",
    "        We then need to zero the gradients of the optimizer for the q_eval network.\n",
    "        We then need to replace the target network.\n",
    "        We then need to get the q values for the current states and the next states.\n",
    "        We then need to get the max q values for the next states.\"\"\"\n",
    "\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            #if memory counter is less than batch size, return\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad() #zero gradients of optimizer for q_eval network (reset gradients)\n",
    "        self.replace_target_network() #replace target network\n",
    "\n",
    "        states, actions, rewards, states_, terminal = self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        #general conversion of data to tensors and send to GPU/CPU\n",
    "        states = T.tensor(np.array(states)).to(self.q_eval.device)  \n",
    "        actions = T.tensor(actions).to(self.q_eval.device) \n",
    "        rewards = T.tensor(rewards).to(self.q_eval.device) \n",
    "        states_ = T.tensor(np.array(states_)).to(self.q_eval.device) \n",
    "        terminal = T.tensor(terminal).to(self.q_eval.device) \n",
    "\n",
    "        # Get the Q-values for the current states\n",
    "        q_eval = self.q_eval.forward(states)\n",
    "\n",
    "        # Select the Q-values corresponding to the taken actions\n",
    "        q_pred = q_eval[T.arange(self.batch_size), actions].to(self.q_eval.device)\n",
    "        q_next_values = self.q_next.forward(states_)\n",
    "        q_next = q_next_values.max(dim=1)[0]\n",
    "\n",
    "        # Get the maximum Q-values for the next states from target network\n",
    "        q_next = self.q_next.forward(states_).max(dim=1)[0]\n",
    "        q_next = q_next.squeeze(1)  # Expand dimensions along the action dimension\n",
    "\n",
    "        # Expand the dimensions of rewards to match the shape of q_next\n",
    "        expanded_rewards = rewards[:, None]\n",
    "\n",
    "        # Set Q-value to 0 for terminal states\n",
    "        q_next[terminal] = 0.0\n",
    "\n",
    "        # Compute the target Q-value\n",
    "        q_target = expanded_rewards + self.gamma * q_next\n",
    "\n",
    "        # Convert to double to prevent overflow\n",
    "        q_target = q_target.double()\n",
    "        q_pred = q_pred.double()\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device) #compute the loss by taking the mean squared error between q_target and q_pred\n",
    "        loss.backward() #backpropagate the loss\n",
    "\n",
    "        # Update the weights of the network\n",
    "        self.q_eval.optimizer.step() #update the weights of the network using the optimizer step\n",
    "        self.learn_step_counter += 1 #increment the learn step counter so we know when to replace the target network\n",
    "        self.decrement_epsilon() #decrement epsilon so that the agent will take less random actions\n",
    "\n",
    "#build a deep q network class\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, lr, name, chkpt_dir):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(*input_dims, 128) #fully connected layer 1 with 128 neurons, 128 is a random number\n",
    "        self.fc2 = nn.Linear(128, 128) #fully connected layer 2 with 128 neurons\n",
    "        self.fc3 = nn.Linear(128, n_actions) #fully connected layer 3 with n_actions neurons\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr) #adam optimizer\n",
    "        self.loss = nn.MSELoss()  # Set Mean squared error loss\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu') #gpu or cpu\n",
    "        self.to(self.device) #move to gpu or cpu\n",
    "  \n",
    "    def forward(self, state): #forward pass\n",
    "        state = state.float() #convert to float\n",
    "        layer1 = F.relu(self.fc1(state)) #relu activation function for layer 1\n",
    "        layer2 = F.relu(self.fc2(layer1)) #relu activation function for layer 2 \n",
    "        return self.fc3(layer2) #return layer 3\n",
    "\n",
    "\n",
    "# Define training parameters\n",
    "lr = 0.001\n",
    "input_dims = [env.observation_space.shape[1]]\n",
    "eps_history = []\n",
    "length = 1000\n",
    "rewards = 0\n",
    "episode_length = 3 # Length of each episode\n",
    "\n",
    "# Create agent and set the number of episodes\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, lr=lr, input_dims=input_dims, n_actions=3, batch_size=64, eps_min=0.01, eps_dec=0.996, replace=100)\n",
    "\n",
    "# Create history object to store data during training\n",
    "history.set(episode_length=length, rewards=rewards)\n",
    "\n",
    "# Training loop\n",
    "for _ in range(episode_length):\n",
    "    done = False\n",
    "    observation_tuple = env.reset()\n",
    "    observation = observation_tuple[0]\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        try:\n",
    "            observation_, reward, done, _, info = env.step(action)\n",
    "            observation_ = observation_[0]\n",
    "            agent.memory.store_transition(np.array(observation), action, reward, np.array(observation_), done)  # Convert to numpy arrays\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "            eps_history.append(agent.epsilon)\n",
    "            history.add(episode_length=episode_length, rewards=reward)\n",
    "        except IndexError:\n",
    "            done = True\n",
    "    env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot episodes vs rewards\n",
    "plt.plot(history['episode_length'], history['rewards'])\n",
    "plt.xlabel('Episode Length')\n",
    "plt.ylabel('Rewards')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
